# Interview Guide: Crypto Volatility Detection Project

## üéØ Project Overview (30-second pitch)

"I built a real-time ML system that predicts 60-second volatility spikes in BTC-USD cryptocurrency trading. The system streams live data from Coinbase via Kafka, computes windowed features in real-time, and uses XGBoost to predict if the next 60 seconds will have unusually high volatility. We achieved 99.97% PR-AUC and implemented full MLOps practices including drift monitoring, experiment tracking, and model versioning."

---

## üèóÔ∏è System Architecture (How It Works)

### High-Level Flow

```
Coinbase WebSocket ‚Üí Kafka ‚Üí Feature Engineering ‚Üí Model Inference ‚Üí Alerts
     (raw ticks)    (stream)   (60s windows)      (XGBoost)      (predictions)
```

### Key Components

1. **Data Ingestion Layer** (`scripts/ws_ingest.py`)
   - Connects to Coinbase WebSocket API
   - Streams BTC-USD ticker data in real-time
   - Publishes to Kafka topic `ticks.raw`
   - Also archives to NDJSON for reproducibility

2. **Feature Engineering Layer** (`features/featurizer.py`)
   - Kafka consumer that processes raw ticks
   - Maintains 60-second sliding window
   - Computes 5 features every new tick:
     - Midprice return mean/std (volatility proxy)
     - Bid-ask spread
     - Trade intensity (trades/sec)
     - Order-book imbalance
   - Publishes features to Kafka topic `ticks.features`
   - Saves to Parquet for batch processing

3. **Model Training** (`models/train.py`)
   - Time-based train/val/test splits (70/15/15)
   - Trains 3 models: baseline (Z-score), Logistic Regression, XGBoost
   - Logs everything to MLflow (params, metrics, artifacts)
   - Forward-looking labels: predicts NEXT 60-second window

4. **Monitoring** (Evidently AI)
   - Data drift detection (early vs late windows)
   - Train vs test distribution comparison
   - Data quality checks

---

## üí° Key Technical Decisions & Why

### 1. **Why Kafka for Data Streaming?**

**Decision**: Use Kafka as message broker between ingestion and feature engineering

**Why**:
- **Decoupling**: Ingestion and feature engineering run independently
- **Scalability**: Can add multiple consumers for different features
- **Reliability**: Kafka handles backpressure and message persistence
- **Replay**: Can reprocess data from Kafka logs if needed

**Interview talking point**: "I chose Kafka because it provides a robust, scalable pipeline that separates concerns. The ingestion service can run continuously while feature engineering can be scaled independently. This is similar to how you'd handle data pipelines for LLM training - you want to decouple data collection from preprocessing."

### 2. **Why Sliding Window Features?**

**Decision**: Compute features over 60-second sliding windows

**Why**:
- **Temporal context**: Volatility is a temporal concept - need historical context
- **Real-time**: Each new tick creates a new window (100% overlap)
- **Efficiency**: Use deque for O(1) append/remove operations

**Interview talking point**: "I implemented a sliding window approach where each new data point creates a new 60-second window. This gives us temporal context while maintaining real-time processing. The window is implemented with a deque for O(1) operations, which is critical for high-frequency data."

### 3. **Why Forward-Looking Labels?**

**Decision**: Use `.shift(-1)` to predict NEXT window's volatility, not current

**Why**:
- **True prediction**: We want to predict future volatility, not detect current
- **Business value**: Gives traders time to react before volatility hits
- **Prevents leakage**: Current window features can't see future volatility

**Interview talking point**: "This is a critical design decision - I use forward-looking labels where we predict the NEXT 60-second window's volatility, not the current one. This prevents data leakage and provides actual predictive value. The label is created by shifting the volatility proxy forward by one window using `.shift(-1)`."

### 4. **Why Time-Based Splits?**

**Decision**: Split data chronologically (70/15/15) instead of random

**Why**:
- **Temporal order**: Financial data has temporal dependencies
- **Realistic evaluation**: Simulates how model will perform on future data
- **Prevents leakage**: Training data can't see future patterns

**Interview talking point**: "I used chronological splits instead of random splits because financial time-series data has temporal dependencies. Random splits would leak future information into training, giving unrealistic performance. This is similar to how you'd split LLM training data - you want to ensure the model generalizes to future data, not just memorizes past patterns."

### 5. **Why Multiple Models?**

**Decision**: Train baseline (Z-score), Logistic Regression, and XGBoost

**Why**:
- **Baseline comparison**: Z-score provides interpretable baseline
- **Linear model**: LR shows if problem is linearly separable
- **Non-linear model**: XGBoost captures complex patterns
- **Model selection**: Compare performance to choose best

**Interview talking point**: "I trained three models to understand the problem complexity. The baseline Z-score rule gives us an interpretable benchmark. Logistic Regression tests if the problem is linearly separable. XGBoost captures non-linear relationships. This systematic approach helps us understand what the data requires."

### 6. **Why Evidently for Monitoring?**

**Decision**: Use Evidently AI for drift detection

**Why**:
- **Distribution shifts**: Market regimes change over time
- **Data quality**: Detect missing values, duplicates, outliers
- **Automated**: Can be integrated into CI/CD pipeline
- **Visualization**: HTML reports for stakeholders

**Interview talking point**: "I implemented drift monitoring with Evidently AI because financial markets have changing regimes. The same model that works in calm markets might fail in volatile periods. I compare early vs late data windows to detect distribution shifts. This is critical for production ML systems - similar to how you'd monitor data quality in LLM training pipelines."

---

## üîß Technical Deep Dives

### Feature Engineering Details

**What features did you compute?**

1. **Midprice Returns** (mean and std)
   - Formula: `log(midprice_t / midprice_{t-1})`
   - Std deviation is our volatility proxy
   - Why: Standard measure of volatility in finance

2. **Bid-Ask Spread**
   - Average spread in window
   - Why: Higher spread = lower liquidity = potential volatility

3. **Trade Intensity**
   - Trades per second
   - Why: Increased activity often precedes volatility

4. **Order-Book Imbalance**
   - Spread normalized by midprice
   - Why: Proxy for order book depth imbalance

**Interview talking point**: "I engineered 5 features from raw ticker data. The most important is the standard deviation of midprice returns, which is a standard volatility proxy in finance. I also included bid-ask spread and trade intensity to capture market microstructure signals. All features are computed over 60-second windows using a sliding window approach."

### Threshold Selection Process

**How did you choose the volatility threshold?**

1. **Initial EDA**: Started with 43 samples, selected 99th percentile
2. **Problem**: Too few samples, threshold miscalibrated
3. **Data collection**: Collected 30 minutes (~33K samples)
4. **Recalibration**: 99th percentile too strict (0.2% positives)
5. **Final choice**: 95th percentile (œÑ = 0.000028) for 5% positives
6. **Justification**: Balance between sensitivity and class distribution

**Interview talking point**: "Threshold selection was iterative. I started with limited data and had to recalibrate as I collected more. The key insight was balancing statistical rigor (high percentile) with practical constraints (sufficient positive examples for training). I used percentile analysis in a Jupyter notebook to visualize the trade-offs."

### Model Training Pipeline

**How does the training pipeline work?**

1. **Data loading**: Read features from Parquet
2. **Label creation**: Forward-looking labels using `.shift(-1)`
3. **Splitting**: Chronological 70/15/15 split
4. **Training**: Train each model with class weights for imbalance
5. **Evaluation**: Compute PR-AUC, F1, precision, recall on all splits
6. **Logging**: Log params, metrics, artifacts to MLflow
7. **Artifacts**: Save models, confusion matrices, PR curves

**Interview talking point**: "The training pipeline is fully automated with MLflow integration. I log all hyperparameters, metrics, and artifacts for reproducibility. The pipeline handles class imbalance using balanced class weights. All models are evaluated on train, validation, and test sets to check for overfitting."

### Inference & Latency

**How does inference work?**

1. **Model loading**: Load from pickle file or MLflow
2. **Feature extraction**: Extract 5 features from input data
3. **Prediction**: Model predicts probability of volatility spike
4. **Latency check**: Verify inference < 120 seconds (2x window size)

**Interview talking point**: "Inference is optimized for real-time use. I implemented latency measurement to ensure predictions complete in under 120 seconds for a batch. The baseline model achieves this easily - inference takes less than 0.1 seconds for 5,000 samples. This is critical for real-time trading applications."

---

## üé§ Interview Talking Points by Topic

### Data Engineering

**Q: "Tell me about the data pipeline"**

**Answer**: "I built a streaming data pipeline using Kafka. Data flows from Coinbase WebSocket ‚Üí Kafka ‚Üí Feature Engineering ‚Üí Model Inference. The key design is decoupling: ingestion runs independently from feature engineering. I also implemented a replay mechanism - raw data is archived to NDJSON, so we can regenerate features identically for reproducibility. This is similar to how you'd handle data pipelines for LLM training - you want versioned, reproducible data processing."

**Key points to mention**:
- Kafka for decoupling and scalability
- Replay mechanism for reproducibility
- Parquet for efficient storage
- Time-based windowing for temporal features

### Data Quality & Monitoring

**Q: "How do you ensure data quality?"**

**Answer**: "I implemented comprehensive monitoring with Evidently AI. I generate drift reports comparing early vs late data windows to detect distribution shifts. I also check for missing values, duplicates, and outliers. The reports are automated and can be integrated into CI/CD. For LLM training, this is critical - you need to detect when your training data distribution shifts, which would degrade model performance."

**Key points to mention**:
- Evidently AI for automated monitoring
- Early vs late window comparison
- Train vs test distribution checks
- Automated report generation

### Model Development

**Q: "Walk me through your modeling approach"**

**Answer**: "I took a systematic approach: baseline ‚Üí simple model ‚Üí complex model. First, I built a Z-score baseline for interpretability. Then Logistic Regression to test linear separability. Finally XGBoost for non-linear patterns. I used time-based splits to prevent data leakage and forward-looking labels for true prediction. All experiments are logged to MLflow for reproducibility. The XGBoost model achieved 99.97% PR-AUC, significantly outperforming the baseline."

**Key points to mention**:
- Systematic model selection
- Time-based splits (no leakage)
- Forward-looking labels
- MLflow for experiment tracking
- Performance metrics (PR-AUC focus)

### Production Readiness

**Q: "How would you deploy this in production?"**

**Answer**: "The system is containerized with Docker. The ingestion service runs in a container, making it easy to deploy. For production, I'd add: 1) Model serving API (Flask/FastAPI), 2) Automated retraining pipeline, 3) A/B testing framework, 4) Alerting for drift detection, 5) Monitoring dashboard. The latency is already verified (< 120s), and the model artifacts are versioned in MLflow. For LLM post-training, similar principles apply - you need versioned models, monitoring, and automated pipelines."

**Key points to mention**:
- Docker containerization
- Model versioning (MLflow)
- Latency verification
- Monitoring infrastructure
- Automated pipelines

### Challenges & Solutions

**Q: "What challenges did you face?"**

**Answer**: "Several key challenges: 1) **Threshold calibration** - Initial threshold from small dataset was miscalibrated. Solution: Collected more data and used percentile analysis. 2) **Class imbalance** - Only 5% positive examples. Solution: Used balanced class weights and adjusted threshold. 3) **Data leakage** - Had to ensure forward-looking labels. Solution: Used `.shift(-1)` and chronological splits. 4) **Reproducibility** - Needed identical features across runs. Solution: Replay script that regenerates features from saved raw data."

**Key points to mention**:
- Threshold calibration iteration
- Handling class imbalance
- Preventing data leakage
- Ensuring reproducibility

---

## üìä Project Metrics to Mention

- **Data**: 33,881 samples collected over 30 minutes
- **Features**: 5 windowed features over 60-second windows
- **Models**: 3 models trained (baseline, LR, XGBoost)
- **Performance**: XGBoost PR-AUC = 0.9997, F1 = 0.9882
- **Latency**: < 0.1 seconds for 5,000 samples (well under 120s requirement)
- **Monitoring**: Automated drift detection with Evidently
- **Tracking**: All experiments logged to MLflow

---

## üéØ Relevance to LLM Post-Training Data SWE Role

### Similarities to LLM Data Work

1. **Data Pipelines**: Streaming data processing, feature engineering
2. **Data Quality**: Monitoring, drift detection, quality checks
3. **Reproducibility**: Versioned data, replay mechanisms
4. **Scalability**: Kafka for distributed processing
5. **Monitoring**: Automated quality and drift detection
6. **Experiment Tracking**: MLflow for model/data versioning

### How to Connect to LLM Work

**Example connections**:
- "The data quality monitoring I implemented is similar to what you'd need for LLM training data - detecting distribution shifts that would degrade model performance."
- "The replay mechanism ensures we can regenerate features identically, which is critical for LLM data preprocessing pipelines."
- "The time-based splits prevent data leakage, similar to how you'd split LLM training data to ensure generalization."
- "The feature engineering pipeline processes streaming data, similar to how you'd process large-scale text data for LLM training."

---

## üó£Ô∏è Practice Questions

### 1. "Tell me about this project" (2-minute answer)

**Structure**:
1. **What**: Real-time ML system for crypto volatility prediction
2. **Why**: Help traders react faster to market swings
3. **How**: Kafka streaming ‚Üí Feature engineering ‚Üí XGBoost model
4. **Results**: 99.97% PR-AUC, < 0.1s latency
5. **Key learnings**: Data quality monitoring, preventing leakage, production ML

### 2. "What was the hardest part?"

**Answer**: "The hardest part was ensuring no data leakage while maintaining real-time performance. I had to carefully design forward-looking labels using `.shift(-1)` and use chronological splits. I also had to iterate on threshold selection as I collected more data. The key was systematic debugging - checking each step of the pipeline to ensure correctness."

### 3. "How would you improve this?"

**Answer**: "Several improvements: 1) Add more features (order book depth, volume profiles), 2) Ensemble multiple models, 3) Online learning to adapt to market changes, 4) Real-time serving API, 5) Automated retraining pipeline, 6) More sophisticated drift detection with automated alerts."

### 4. "What did you learn?"

**Answer**: "Key learnings: 1) Data quality is critical - small distribution shifts can break models, 2) Preventing leakage requires careful design, 3) Production ML needs monitoring and versioning, 4) Simple baselines are important for comparison, 5) Real-time systems need latency verification."

---

## üìù Quick Reference: Technical Terms

- **Sliding Window**: Maintains last 60 seconds of data, updates with each new tick
- **Forward-Looking Labels**: Predicts NEXT window's volatility, not current
- **Chronological Split**: Time-based split to prevent data leakage
- **PR-AUC**: Precision-Recall Area Under Curve (primary metric for imbalanced data)
- **Data Drift**: Distribution changes over time (detected with Evidently)
- **Replay Mechanism**: Regenerate features from saved raw data identically
- **MLflow**: Experiment tracking and model versioning
- **Kafka**: Distributed message broker for streaming data

---

## ‚úÖ Final Checklist Before Interview

- [ ] Can explain the end-to-end pipeline in 2 minutes
- [ ] Understand why each technical decision was made
- [ ] Can discuss data quality and monitoring
- [ ] Can explain how to prevent data leakage
- [ ] Can connect project to LLM data work
- [ ] Know the key metrics (PR-AUC, latency, etc.)
- [ ] Can discuss challenges and solutions
- [ ] Can suggest improvements

---

**Remember**: You built this! Even if I helped with code, you made the decisions, understood the requirements, and iterated on the solution. Focus on the **why** and **what** you learned, not just the **how**.

Good luck with your interview! üöÄ

