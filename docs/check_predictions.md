# How to Check Predictions

This guide shows you multiple ways to verify that predictions are being generated by the Kafka consumer.

## Quick Start

### 1. Start All Services

```bash
cd /Users/saikarthik/cursor/crypto-volatility
docker-compose -f docker/compose.yaml up -d
```

Verify all services are running:
```bash
docker-compose -f docker/compose.yaml ps
```

You should see:
- `kafka` (port 9092)
- `mlflow` (port 5000)
- `volatility-api` (port 8000) - FastAPI
- `volatility-prediction-consumer` - Kafka consumer

### 2. Replay Data to Trigger Predictions

The consumer needs features in the `ticks.features` topic. You can either:

**Option A: Replay existing data**
```bash
python scripts/replay_to_kafka.py --duration 10
```

**Option B: Use existing features (if you have a feature engine running)**
If you have a feature engine that's already producing features, the consumer will automatically process them.

## Methods to Check Predictions

### Method 1: View Kafka Predictions Topic (Recommended)

View predictions directly from the Kafka topic:

```bash
# View predictions in real-time
docker exec -it kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic ticks.predictions \
  --from-beginning \
  --property print.key=true \
  --property print.timestamp=true
```

This will show JSON messages like:
```json
{
  "timestamp": 1234567890.5,
  "product_id": "BTC-USD",
  "price": 50000.0,
  "midprice": 50000.5,
  "prediction": 0,
  "probability": 0.23,
  "threshold": 0.5,
  "features": {
    "midprice_return_mean": 0.00165,
    "midprice_return_std": 0.0015,
    "bid_ask_spread": 1.0,
    "trade_intensity": 2.5,
    "order_book_imbalance": 0.1
  },
  "model_version": "local-1234567890"
}
```

**To see only recent predictions (not from beginning):**
```bash
docker exec -it kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic ticks.predictions
```

### Method 2: Check Consumer Logs

View the consumer's processing logs:

```bash
# View logs in real-time
docker logs -f volatility-prediction-consumer

# View last 50 lines
docker logs --tail 50 volatility-prediction-consumer
```

You should see messages like:
```
✅ Model loaded successfully (version: local-1234567890, source: local_pickle)
✅ Prediction consumer started. Waiting for messages...
Processed 100 messages, made 100 predictions (10.5 predictions/sec)
```

### Method 3: Use FastAPI Endpoint

Test predictions via HTTP (for ad-hoc testing):

```bash
# Make a prediction request
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{
    "price": 50000.0,
    "midprice": 50000.5,
    "return_1s": 0.0001,
    "return_5s": 0.0005,
    "return_30s": 0.002,
    "return_60s": 0.004,
    "volatility": 0.001,
    "trade_intensity": 2.5,
    "spread_abs": 1.0,
    "spread_rel": 0.00002,
    "order_book_imbalance": 0.1
  }'
```

Response:
```json
{
  "prediction": 0,
  "probability": 0.23,
  "model_version": "local-1234567890",
  "timestamp": 1234567890.5
}
```

### Method 4: Check Kafka Topics

Verify the pipeline is working by checking all topics:

```bash
# List all topics
docker exec -it kafka kafka-topics --list --bootstrap-server localhost:9092

# Check message counts
docker exec -it kafka kafka-run-class kafka.tools.GetOffsetShell \
  --broker-list localhost:9092 \
  --topic ticks.raw

docker exec -it kafka kafka-run-class kafka.tools.GetOffsetShell \
  --broker-list localhost:9092 \
  --topic ticks.features

docker exec -it kafka kafka-run-class kafka.tools.GetOffsetShell \
  --broker-list localhost:9092 \
  --topic ticks.predictions
```

### Method 5: Monitor Consumer Group

Check the consumer group status:

```bash
# Describe consumer group
docker exec -it kafka kafka-consumer-groups \
  --bootstrap-server localhost:9092 \
  --group volatility-predictor \
  --describe
```

This shows:
- Current offset
- Lag (messages behind)
- Partition assignments

### Method 6: Create a Simple Prediction Viewer Script

Create a Python script to consume and display predictions:

```python
# scripts/view_predictions.py
import json
import sys
from confluent_kafka import Consumer
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))
import yaml

with open('config.yaml', 'r') as f:
    config = yaml.safe_load(f)

consumer = Consumer({
    'bootstrap.servers': config['kafka']['bootstrap_servers'],
    'group.id': 'prediction-viewer',
    'auto.offset.reset': 'latest'
})

consumer.subscribe([config['kafka']['topic_predictions']])

print("Waiting for predictions... (Press Ctrl+C to stop)")
try:
    while True:
        msg = consumer.poll(timeout=1.0)
        if msg is None:
            continue
        if msg.error():
            print(f"Error: {msg.error()}")
            continue
        
        prediction = json.loads(msg.value().decode('utf-8'))
        print(f"\n[{prediction['timestamp']}] {prediction['product_id']}")
        print(f"  Prediction: {prediction['prediction']} (spike={'YES' if prediction['prediction']==1 else 'NO'})")
        print(f"  Probability: {prediction['probability']:.4f}")
        print(f"  Price: ${prediction['price']:.2f}")
except KeyboardInterrupt:
    print("\nStopped.")
finally:
    consumer.close()
```

Run it:
```bash
python scripts/view_predictions.py
```

## Troubleshooting

### No Predictions Appearing

1. **Check consumer is running:**
   ```bash
   docker ps | grep prediction-consumer
   ```

2. **Check consumer logs for errors:**
   ```bash
   docker logs volatility-prediction-consumer
   ```

3. **Verify features topic has messages:**
   ```bash
   docker exec -it kafka kafka-console-consumer \
     --bootstrap-server localhost:9092 \
     --topic ticks.features \
     --max-messages 1
   ```

4. **Check model is loaded:**
   Look for "✅ Model loaded successfully" in consumer logs

5. **Verify consumer group:**
   ```bash
   docker exec -it kafka kafka-consumer-groups \
     --bootstrap-server localhost:9092 \
     --group volatility-predictor \
     --describe
   ```

### Consumer Not Processing Messages

- **Check offset:** Consumer might be at the end of the topic. Reset it:
  ```bash
  docker exec -it kafka kafka-consumer-groups \
    --bootstrap-server localhost:9092 \
    --group volatility-predictor \
    --reset-offsets \
    --to-earliest \
    --topic ticks.features \
    --execute
  ```

- **Restart consumer:**
  ```bash
  docker-compose -f docker/compose.yaml restart prediction-consumer
  ```

### Model Loading Errors

- **Check model file exists:**
  ```bash
  ls -lh models/artifacts/xgb_model.pkl
  ```

- **Check MLflow connection:**
  ```bash
  curl http://localhost:5000/health
  ```

## Expected Output

When everything is working, you should see:

1. **Consumer logs showing:**
   - Model loaded successfully
   - Messages processed
   - Predictions made
   - Processing rate

2. **Kafka topic `ticks.predictions` containing:**
   - JSON messages with predictions
   - One message per feature message consumed

3. **FastAPI `/predict` endpoint:**
   - Returns predictions for ad-hoc requests
   - Shows model version and probability

## Quick Test Command

Run this to see predictions in real-time:

```bash
# Terminal 1: Start services
docker-compose -f docker/compose.yaml up -d

# Terminal 2: Replay data (triggers features and predictions)
python scripts/replay_to_kafka.py --duration 5

# Terminal 3: Watch predictions
docker exec -it kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic ticks.predictions \
  --from-beginning | jq .
```

Note: `jq .` pretty-prints JSON. Install with `brew install jq` on macOS.

